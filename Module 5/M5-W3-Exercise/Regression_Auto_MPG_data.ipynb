{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQO5ZV17oeuS",
    "outputId": "bb81eceb-024f-41fd-84f3-aabfb1c3744f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1qiUDDoYyRLBiKOoYWdFl_5WByHE8Cugu\n",
      "To: /content/Auto_MPG_data.csv\n",
      "100% 15.4k/15.4k [00:00<00:00, 29.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "! gdown --id 1qiUDDoYyRLBiKOoYWdFl_5WByHE8Cugu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RpU5-mTlogii"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H0i8NMLYpCwI"
   },
   "outputs": [],
   "source": [
    "random_state = 59\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Vjte9iuvpDFQ"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "990dIi0SpHWH"
   },
   "outputs": [],
   "source": [
    "dataset_path = '/content/Auto_MPG_data.csv'\n",
    "dataset = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SgDVZbX8pKoU"
   },
   "outputs": [],
   "source": [
    "X = dataset.drop(columns='MPG').values\n",
    "y = dataset['MPG'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fu4v7bzppQ3t"
   },
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "test_size = 0.125\n",
    "is_shuffle = True\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=val_size,\n",
    "    random_state=random_state,\n",
    "    shuffle=is_shuffle\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    shuffle=is_shuffle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MBD6OUOEpZwI"
   },
   "outputs": [],
   "source": [
    "normalizer = StandardScaler()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_val = normalizer.transform(X_val)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gjhGLLXQpjyA"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZebD5oOkpn4f"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "snq2KhERppDc"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.linear2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        out = self.output(x)\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2H31haf6pt_V"
   },
   "outputs": [],
   "source": [
    "input_dims = X_train.shape[1]\n",
    "output_dims = 1\n",
    "hidden_dims = 64\n",
    "\n",
    "model = MLP(input_dims=input_dims, hidden_dims=hidden_dims, output_dims=output_dims).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "scMipqMSpwmu"
   },
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "svnxN65Op0uK"
   },
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    y_true = torch.Tensor(y_true).to(device)\n",
    "    y_pred = torch.Tensor(y_pred).to(device)\n",
    "    mean_true = torch.mean(y_true)\n",
    "    ss_tot = torch.sum((y_true - mean_true) ** 2)\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIovyeiNp5Pp",
    "outputId": "a7a6b770-4939-48bb-a82b-b2fa08068c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1:  Training loss: 282.769   Validation loss: 88.672\n",
      "\n",
      "EPOCH 2:  Training loss: 137.669   Validation loss: 72.346\n",
      "\n",
      "EPOCH 3:  Training loss: 71.007   Validation loss: 19.143\n",
      "\n",
      "EPOCH 4:  Training loss: 25.083   Validation loss: 196.176\n",
      "\n",
      "EPOCH 5:  Training loss: 96.139   Validation loss: 20.444\n",
      "\n",
      "EPOCH 6:  Training loss: 17.765   Validation loss: 9.444\n",
      "\n",
      "EPOCH 7:  Training loss: 18.486   Validation loss: 14.535\n",
      "\n",
      "EPOCH 8:  Training loss: 37.859   Validation loss: 37.427\n",
      "\n",
      "EPOCH 9:  Training loss: 17.133   Validation loss: 38.134\n",
      "\n",
      "EPOCH 10:  Training loss: 22.991   Validation loss: 41.183\n",
      "\n",
      "EPOCH 11:  Training loss: 26.723   Validation loss: 20.063\n",
      "\n",
      "EPOCH 12:  Training loss: 9.852   Validation loss: 5.594\n",
      "\n",
      "EPOCH 13:  Training loss: 15.143   Validation loss: 16.025\n",
      "\n",
      "EPOCH 14:  Training loss: 12.213   Validation loss: 12.023\n",
      "\n",
      "EPOCH 15:  Training loss: 14.222   Validation loss: 7.731\n",
      "\n",
      "EPOCH 16:  Training loss: 10.845   Validation loss: 18.904\n",
      "\n",
      "EPOCH 17:  Training loss: 12.312   Validation loss: 14.885\n",
      "\n",
      "EPOCH 18:  Training loss: 15.474   Validation loss: 12.354\n",
      "\n",
      "EPOCH 19:  Training loss: 13.783   Validation loss: 5.380\n",
      "\n",
      "EPOCH 20:  Training loss: 7.285   Validation loss: 5.100\n",
      "\n",
      "EPOCH 21:  Training loss: 8.168   Validation loss: 4.832\n",
      "\n",
      "EPOCH 22:  Training loss: 6.277   Validation loss: 5.236\n",
      "\n",
      "EPOCH 23:  Training loss: 10.050   Validation loss: 50.349\n",
      "\n",
      "EPOCH 24:  Training loss: 10.607   Validation loss: 15.387\n",
      "\n",
      "EPOCH 25:  Training loss: 9.235   Validation loss: 8.590\n",
      "\n",
      "EPOCH 26:  Training loss: 9.764   Validation loss: 6.845\n",
      "\n",
      "EPOCH 27:  Training loss: 6.272   Validation loss: 6.215\n",
      "\n",
      "EPOCH 28:  Training loss: 10.287   Validation loss: 5.041\n",
      "\n",
      "EPOCH 29:  Training loss: 6.383   Validation loss: 11.531\n",
      "\n",
      "EPOCH 30:  Training loss: 17.039   Validation loss: 4.781\n",
      "\n",
      "EPOCH 31:  Training loss: 7.879   Validation loss: 5.786\n",
      "\n",
      "EPOCH 32:  Training loss: 6.630   Validation loss: 6.206\n",
      "\n",
      "EPOCH 33:  Training loss: 7.156   Validation loss: 4.626\n",
      "\n",
      "EPOCH 34:  Training loss: 6.758   Validation loss: 6.483\n",
      "\n",
      "EPOCH 35:  Training loss: 7.450   Validation loss: 6.463\n",
      "\n",
      "EPOCH 36:  Training loss: 5.831   Validation loss: 5.967\n",
      "\n",
      "EPOCH 37:  Training loss: 5.947   Validation loss: 6.008\n",
      "\n",
      "EPOCH 38:  Training loss: 7.273   Validation loss: 10.705\n",
      "\n",
      "EPOCH 39:  Training loss: 7.611   Validation loss: 14.612\n",
      "\n",
      "EPOCH 40:  Training loss: 7.478   Validation loss: 5.625\n",
      "\n",
      "EPOCH 41:  Training loss: 5.840   Validation loss: 20.782\n",
      "\n",
      "EPOCH 42:  Training loss: 8.010   Validation loss: 5.530\n",
      "\n",
      "EPOCH 43:  Training loss: 7.096   Validation loss: 28.658\n",
      "\n",
      "EPOCH 44:  Training loss: 10.015   Validation loss: 10.240\n",
      "\n",
      "EPOCH 45:  Training loss: 6.435   Validation loss: 4.911\n",
      "\n",
      "EPOCH 46:  Training loss: 8.238   Validation loss: 7.057\n",
      "\n",
      "EPOCH 47:  Training loss: 6.315   Validation loss: 6.785\n",
      "\n",
      "EPOCH 48:  Training loss: 5.323   Validation loss: 6.720\n",
      "\n",
      "EPOCH 49:  Training loss: 6.652   Validation loss: 9.927\n",
      "\n",
      "EPOCH 50:  Training loss: 6.583   Validation loss: 12.207\n",
      "\n",
      "EPOCH 51:  Training loss: 8.147   Validation loss: 7.704\n",
      "\n",
      "EPOCH 52:  Training loss: 10.966   Validation loss: 5.014\n",
      "\n",
      "EPOCH 53:  Training loss: 7.193   Validation loss: 4.809\n",
      "\n",
      "EPOCH 54:  Training loss: 6.586   Validation loss: 5.283\n",
      "\n",
      "EPOCH 55:  Training loss: 5.428   Validation loss: 5.254\n",
      "\n",
      "EPOCH 56:  Training loss: 5.410   Validation loss: 6.241\n",
      "\n",
      "EPOCH 57:  Training loss: 7.397   Validation loss: 7.304\n",
      "\n",
      "EPOCH 58:  Training loss: 6.877   Validation loss: 19.924\n",
      "\n",
      "EPOCH 59:  Training loss: 7.607   Validation loss: 6.910\n",
      "\n",
      "EPOCH 60:  Training loss: 5.577   Validation loss: 4.655\n",
      "\n",
      "EPOCH 61:  Training loss: 5.597   Validation loss: 9.228\n",
      "\n",
      "EPOCH 62:  Training loss: 11.281   Validation loss: 4.500\n",
      "\n",
      "EPOCH 63:  Training loss: 6.602   Validation loss: 13.699\n",
      "\n",
      "EPOCH 64:  Training loss: 6.271   Validation loss: 6.322\n",
      "\n",
      "EPOCH 65:  Training loss: 8.267   Validation loss: 5.209\n",
      "\n",
      "EPOCH 66:  Training loss: 5.305   Validation loss: 5.244\n",
      "\n",
      "EPOCH 67:  Training loss: 5.974   Validation loss: 6.119\n",
      "\n",
      "EPOCH 68:  Training loss: 10.930   Validation loss: 7.538\n",
      "\n",
      "EPOCH 69:  Training loss: 7.000   Validation loss: 9.098\n",
      "\n",
      "EPOCH 70:  Training loss: 6.188   Validation loss: 5.058\n",
      "\n",
      "EPOCH 71:  Training loss: 5.184   Validation loss: 4.528\n",
      "\n",
      "EPOCH 72:  Training loss: 5.927   Validation loss: 9.171\n",
      "\n",
      "EPOCH 73:  Training loss: 6.805   Validation loss: 9.461\n",
      "\n",
      "EPOCH 74:  Training loss: 6.086   Validation loss: 5.240\n",
      "\n",
      "EPOCH 75:  Training loss: 5.313   Validation loss: 16.254\n",
      "\n",
      "EPOCH 76:  Training loss: 7.395   Validation loss: 18.961\n",
      "\n",
      "EPOCH 77:  Training loss: 10.982   Validation loss: 7.603\n",
      "\n",
      "EPOCH 78:  Training loss: 6.881   Validation loss: 5.415\n",
      "\n",
      "EPOCH 79:  Training loss: 5.829   Validation loss: 10.532\n",
      "\n",
      "EPOCH 80:  Training loss: 8.031   Validation loss: 5.808\n",
      "\n",
      "EPOCH 81:  Training loss: 10.559   Validation loss: 5.661\n",
      "\n",
      "EPOCH 82:  Training loss: 5.536   Validation loss: 5.436\n",
      "\n",
      "EPOCH 83:  Training loss: 5.733   Validation loss: 8.689\n",
      "\n",
      "EPOCH 84:  Training loss: 5.447   Validation loss: 5.063\n",
      "\n",
      "EPOCH 85:  Training loss: 4.924   Validation loss: 6.608\n",
      "\n",
      "EPOCH 86:  Training loss: 4.733   Validation loss: 6.193\n",
      "\n",
      "EPOCH 87:  Training loss: 5.094   Validation loss: 7.785\n",
      "\n",
      "EPOCH 88:  Training loss: 5.322   Validation loss: 4.985\n",
      "\n",
      "EPOCH 89:  Training loss: 4.837   Validation loss: 5.246\n",
      "\n",
      "EPOCH 90:  Training loss: 6.566   Validation loss: 5.694\n",
      "\n",
      "EPOCH 91:  Training loss: 5.474   Validation loss: 8.742\n",
      "\n",
      "EPOCH 92:  Training loss: 7.184   Validation loss: 5.376\n",
      "\n",
      "EPOCH 93:  Training loss: 6.221   Validation loss: 5.282\n",
      "\n",
      "EPOCH 94:  Training loss: 9.373   Validation loss: 7.654\n",
      "\n",
      "EPOCH 95:  Training loss: 6.485   Validation loss: 5.797\n",
      "\n",
      "EPOCH 96:  Training loss: 8.487   Validation loss: 22.967\n",
      "\n",
      "EPOCH 97:  Training loss: 7.113   Validation loss: 11.046\n",
      "\n",
      "EPOCH 98:  Training loss: 7.269   Validation loss: 7.265\n",
      "\n",
      "EPOCH 99:  Training loss: 5.988   Validation loss: 5.091\n",
      "\n",
      "EPOCH 100:  Training loss: 6.082   Validation loss: 4.776\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_r2 = []\n",
    "val_r2 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    train_target = []\n",
    "    val_target = []\n",
    "    train_predict = []\n",
    "    val_predict = []\n",
    "\n",
    "    model.train()\n",
    "    for X_samples, y_samples in train_loader:\n",
    "        X_samples = X_samples.to(device)\n",
    "        y_samples = y_samples.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(X_samples)\n",
    "        train_predict += outputs.tolist()\n",
    "        train_target += y_samples.tolist()\n",
    "\n",
    "        loss = criterion(outputs, y_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_r2.append(r_squared(train_target, train_predict))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_samples, y_samples in val_loader:\n",
    "            X_samples = X_samples.to(device)\n",
    "            y_samples = y_samples.to(device)\n",
    "\n",
    "            outputs = model(X_samples)\n",
    "            val_predict += outputs.tolist()\n",
    "            val_target += y_samples.tolist()\n",
    "\n",
    "            loss = criterion(outputs, y_samples)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_r2.append(r_squared(val_target, val_predict))\n",
    "\n",
    "    print(f'\\nEPOCH {epoch + 1}:  Training loss: {train_loss:.3f}   Validation loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWo-93AVqAgs",
    "outputId": "a3eac8ce-4b52-411c-dfa1-c3499b24ea98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test set:\n",
      "R2: 0.8417800068855286\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model(X_test)\n",
    "    test_set_r2 = r_squared(y_hat, y_test)\n",
    "\n",
    "print('Evaluation on test set:')\n",
    "print(f'R2: {test_set_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqhkeFjNqw0-"
   },
   "source": [
    "### **Bài tập**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpmCtFqmqzmu"
   },
   "source": [
    "Khi điều chỉnh kiến trúc class “MLP” thành dạng Linear Regression và với cùng\n",
    "một cài đặt tham số như phiên bản gốc, kết quả của mô hình đã huấn luyện được\n",
    "trên tập test có điểm R2 xấp xỉ bằng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "AbLxC1AvqzIq"
   },
   "outputs": [],
   "source": [
    "class MLP1(nn.Module):\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.output = nn.Linear(input_dims, output_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.output(x)\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MHD4ge4rQTo",
    "outputId": "5cab8caf-0264-41f7-cfee-e4188f4942f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test set:\n",
      "R2: 0.775617241859436\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model(X_test)\n",
    "    test_set_r2 = r_squared(y_hat, y_test)\n",
    "\n",
    "print('Evaluation on test set:')\n",
    "print(f'R2: {test_set_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ESQjtB4ryut"
   },
   "source": [
    "Khi điều chỉnh các hàm kích hoạt được sử dụng trong class “MLP” thành hàm Sigmoid và với cùng một cài đặt tham số như phiên bản gốc, kết quả của mô hình đã huấn luyện được trên tập test có điểm R2 xấp xỉ bằng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "u3h2tRTDrTdE"
   },
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.linear2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        out = self.output(x)\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bv7rGGcrsE-M",
    "outputId": "b1528650-0b0d-428d-c9fb-ce7d7d1eb2d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test set:\n",
      "R2: 0.8611322641372681\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model(X_test)\n",
    "    test_set_r2 = r_squared(y_hat, y_test)\n",
    "\n",
    "print('Evaluation on test set:')\n",
    "print(f'R2: {test_set_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWREjhqLsrwx"
   },
   "source": [
    "Khi điều chỉnh các hàm kích hoạt được sử dụng trong class “MLP” thành hàm Tanh\n",
    "và với cùng một cài đặt tham số như phiên bản gốc, kết quả của mô hình đã huấn\n",
    "luyện được trên tập test có điểm R2 xấp xỉ bằng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "MGHEBUacsI1e"
   },
   "outputs": [],
   "source": [
    "class MLP3(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.linear2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.tanh(x)\n",
    "        out = self.output(x)\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbnoaHGstRTd",
    "outputId": "04ff97d6-40b0-4bda-8afa-91c858fcca3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test set:\n",
      "R2: 0.8794631958007812\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model(X_test)\n",
    "    test_set_r2 = r_squared(y_hat, y_test)\n",
    "\n",
    "print('Evaluation on test set:')\n",
    "print(f'R2: {test_set_r2}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
